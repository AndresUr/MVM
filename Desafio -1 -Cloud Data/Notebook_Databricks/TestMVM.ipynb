{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe75b81-18ae-4949-8a36-03e981b6d342",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente notebook de databricks tiene como objetivo la  generacion datos ficticios para el area de departamentos, puestos de trabajo y empleados haciendo uso de pySpark para la integracion con el cluster de spark y databricks, Faker para la generacion de datos dummies,mysql-connector-python y SQLAlchemy para la conexion a una base de datos MYSQL.\n",
    "Una vez generados estos datos ficticios estos son cargados al sistema de almacenamiento en la nube de azure Blob Storage donde son alamcenados en formato parquet.\n",
    "Una vez almacenados esto datos en formato parquet son leidos y convertidos a pandas con el fin de ser insertados en batch a la base de datos MYSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c50fb6-dfd3-4bff-afeb-8b37847211be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A continuacion instalamos las librerias a utilizar,\n",
    "Faker: genracion de datos Dummies\n",
    "mysql-connector y SQLAlchemy se utilizan con el fin de establecer la conexion a la base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3baf9eb-f1b7-4549-839f-1d3ce1ab61b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting faker\n",
      "  Downloading Faker-24.14.0-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-24.14.0\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.3.0-cp39-cp39-manylinux_2_17_x86_64.whl (21.5 MB)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.3.0\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Collecting SQLAlchemy\n",
      "  Downloading SQLAlchemy-2.0.29-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
      "Collecting typing-extensions>=4.6.0\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: typing-extensions, greenlet, SQLAlchemy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-69774542-4551-47f1-800d-7583dbc91d9d\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "Successfully installed SQLAlchemy-2.0.29 greenlet-3.0.3 typing-extensions-4.11.0\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker\n",
    "%pip install mysql-connector-python\n",
    "%pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ec69fb-71ed-42a9-a754-9418eebdde89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aquí importamos las librerias  necesarias para nuestro script. Utilizamos pyspark.sql para trabajar con Spark DataFrame, Faker para generar datos dummies y random para generar números aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f549f0d-6e2e-433c-8c7d-80c05466d56f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a55731-4cdf-4504-afa6-44caff4813e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8acb4ef1-f9b5-4679-9a72-e3eecca29002",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En esta seccion creamos una sesión de Spark utilizando SparkSession.builder.getOrCreate(), donde esta sesion proporciona una sesion de trabajo con el cluster de Spark.\n",
    "de igual manera configuramos la conexion al Azure Blob Storage, esto con el fin de poder acceder ay escribir los datos en los containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ac93a6-333c-4b11-a742-1a867aa50abc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "session.conf.set(\n",
    "    \"s.azure.account.key.cwsazure.blob.core.windows.net\",\n",
    "    \"oBRTYU3douji12**//==sZTcudddTlQaJdaM6UPZUk5oQ0rGR..-XVVWIpusvHvMBTliZOVwL+AStLxdUwQ==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84310fb7-cf6f-4a1a-837a-a36b4e05b8e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se crea un dataframe de pyspark llamado departmentsdf el cual contiene una serie de datos ficticios acerca de los departamentos o areas presentes en una empresa, usamos un list comprehension para iterar los departamentos ficticios y asi enumerarlos y crear el dataframe con los campos de dep_id y deo_name, donde:\n",
    "\n",
    "* dep_id : representa el id del departamento\n",
    "* dep_nam : representa el nombre del departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c1cac7-28a8-421f-bfa7-5c8b06b6c61b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "departments = [\"Human Resources\", \"Finance\", \"Marketing\", \"Operations\", \"Sales\"]\n",
    "departments_generate = [(i+1, department) for i, department in enumerate(departments)]\n",
    "departmentsdf = spark.createDataFrame(departments_generate, [\"dep_id\", \"dep_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8a6780-c5d3-4ea2-97c5-61c73703689a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|dep_id|       dep_name|\n",
      "+------+---------------+\n",
      "|     1|Human Resources|\n",
      "|     2|        Finance|\n",
      "|     3|      Marketing|\n",
      "|     4|     Operations|\n",
      "|     5|          Sales|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0340de6f-d820-4aee-b249-4a5258595d81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se crea un dataframe de pyspark llamado jobsdf  el cual contiene una serie de datos ficticios acerca de los diferentes puestos de trabajo que hay  presentes en una empresa, en esta ocacion creamos una lista la cual contiene tuplas con los datos de cada puesto de trabajo, los datos presentes en estas tuplas son job_id, job_name, min_salary, max_salary donde:\n",
    "\n",
    "* job_id : representa el id del puesto de trabajo\n",
    "* job_name : representa el nombre del puesto de trabajo\n",
    "* min_salary : representa el salario minimo del puesto de trabajo\n",
    "* max_salary : representa el salario maximo del puesto de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361934d5-d27d-4c39-a699-0fad7dc60dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs = [(1, \"Manager\", 50000, 80000),\n",
    "             (2, \"Developer\", 40000, 60000),\n",
    "             (3, \"Designer\", 35000, 50000),\n",
    "             (4, \"Analyst\", 45000, 70000),\n",
    "             (5, \"Assistant\", 30000, 35000)]\n",
    "jobsdf = spark.createDataFrame(jobs, [\"job_id\", \"job_name\", \"min_salary\", \"max_salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d63e097-cc68-4d2a-b1f1-f065be7d6999",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se crea un dataframe llamado employeesdf, el cual utiliza una list comprehension con el fin de crear una lista de 100 tuplas  donde cada tupla representa un empleado, el cual cuenta con los registros de employee_id, department_id,job_id, first_name, last_name, email,phone_number, contract_date,salary  donde :\n",
    "* employee_id : representa el id del empleado\n",
    "* department_id  : representa el id del departamento al cual pertenece el empleado\n",
    "* job_id : representa el id del puesto de trabajo al cual pertenece el empleado\n",
    "* first_name : representa el primer nombre del empleado, este first_name es generado mediante la libreria fake el cual trae data dummi\n",
    "* last_name : representa el segundo  nombre del empleado, este last_name es generado mediante la libreria fake el cual trae data dummi\n",
    "* email : representa el email del empleado, este email es generado mediante la libreria fake el cual trae data dummi\n",
    "* phone_number  : representa el numero de telefono del empleado,este numero de telefono es generado mediante la libreria fake la cual trae data dummi \n",
    "* contract_date  : representa fecha de contratacion del empleado, esta fecha es generado mediante la libreria fake la cual trae data dummi de fechas\n",
    "* salary : representa el salario que gana el empleado, este salario es generado de manera aleatoria con un rango de valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb67cce-1e26-4a5e-9f80-695767ef8c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "n = 100\n",
    "data = [(k+1,\n",
    "        random.randint(1, len(departments)),\n",
    "        random.randint(1, len(jobs)),\n",
    "        fake.first_name(),\n",
    "        fake.last_name(),\n",
    "        fake.email(),\n",
    "        fake.phone_number(),\n",
    "        fake.date_between(start_date='-5y', end_date='today'),                   \n",
    "        random.randint(30000, 80000)) for k in range(n)]\n",
    "\n",
    "employeesdf = spark.createDataFrame(data, [\"employee_id\", \"department_id\",\"job_id\", \"first_name\", \"last_name\", \"email\",\n",
    "                                                       \"phone_number\", \"contract_date\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe614221-b61d-4675-9ac9-556a5473462b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+----------+---------+--------------------+--------------------+-------------+------+\n",
      "|employee_id|department_id|job_id|first_name|last_name|               email|        phone_number|contract_date|salary|\n",
      "+-----------+-------------+------+----------+---------+--------------------+--------------------+-------------+------+\n",
      "|          1|            5|     3|   Brandon|   Miller|ashleywilcox@exam...|   361.292.3713x6066|   2020-07-08| 54413|\n",
      "|          2|            1|     1|      John|  Osborne|jimenezbrandon@ex...|  752-398-5954x03932|   2020-09-16| 56586|\n",
      "|          3|            2|     3|   Micheal|    Brown| amypham@example.com| (342)281-4004x61433|   2021-01-21| 79661|\n",
      "|          4|            3|     4|      Dean|    Smith|hamiltontom@examp...|        365-210-9127|   2023-05-21| 42101|\n",
      "|          5|            4|     2|      Mark| Holloway|ycampbell@example...|        301-353-2266|   2024-04-11| 63936|\n",
      "|          6|            4|     5|      Adam|  Anthony|andersonlucas@exa...|  (830)867-1970x8598|   2021-06-23| 40616|\n",
      "|          7|            2|     5|    Amanda| Mcdonald|michaelmunoz@exam...| (216)746-8408x70241|   2023-08-03| 63136|\n",
      "|          8|            1|     4|      Alan|  Kennedy|william25@example...|  317-908-8737x05134|   2020-08-25| 53573|\n",
      "|          9|            2|     4|      Juan|    Banks|barbara46@example...|    976.390.3978x909|   2022-06-16| 63701|\n",
      "|         10|            2|     3|  Virginia| Robinson|  dean58@example.org|   819-764-5975x6869|   2022-02-10| 53439|\n",
      "|         11|            5|     2|    Robert|    Ramos|   wwolf@example.net|001-876-831-9176x...|   2022-09-12| 61618|\n",
      "|         12|            4|     1|     Holly|Zimmerman|ballalexander@exa...|        543.635.1215|   2020-07-04| 75081|\n",
      "|         13|            3|     3| Elizabeth|   Martin|rogerpatterson@ex...| (256)515-4572x89578|   2024-03-11| 59653|\n",
      "|         14|            4|     5|    Amanda|    Baker|dannybrown@exampl...|    780-451-5628x700|   2021-02-04| 76870|\n",
      "|         15|            4|     4|      Kurt|    Scott|lawrencearnold@ex...|  218.909.9691x92276|   2024-03-19| 49856|\n",
      "|         16|            3|     5|     Scott|   Wright|yvette01@example.org|        357.349.1757|   2024-02-20| 66172|\n",
      "|         17|            5|     4|   Carolyn|   Wilson|graywilliam@examp...|   858.834.8748x8241|   2024-03-09| 73085|\n",
      "|         18|            3|     1|      Lisa|Donaldson|reneeholmes@examp...| (307)768-9406x86856|   2022-02-18| 67106|\n",
      "|         19|            5|     3|   Garrett|  Vazquez|theresa47@example...|          9382733561|   2021-08-11| 65663|\n",
      "|         20|            5|     2|    Angela|     Lutz|wgalloway@example...|          4823407229|   2021-05-01| 44292|\n",
      "+-----------+-------------+------+----------+---------+--------------------+--------------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96127853-8bd3-4f48-8958-cfb7ebea93ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Escribimos los dataframes generados en formato parquet en el servicio de almacenamiento de azure blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c509f74d-77b0-4994-9ea1-e0fc69b6d111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employeesdf.write.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-employees/\")\n",
    "jobsdf.write.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-jobs/\")\n",
    "departmentsdf.write.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-departments/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb1f886-7a86-4a13-ac9f-27e2561824fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Leemos los archivos tipo parquet en formato de dataframes de pyspark, luego los ordenamos por su id y por ultimo los convertimos a dataframes de pandas esto con el fin de hacer un proceso de insersion de datos en batch a una base de datos MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1786f63-ccb1-48e6-a844-61aa6e0f88f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfemployees = session.read.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-employees/\").orderBy(col(\"employee_id\")).toPandas()\n",
    "dfjobs = session.read.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-jobs/\").orderBy(col(\"job_id\")).toPandas()\n",
    "dfdepartments = session.read.parquet(\"wasbs://data-bronze@cwsazure.blob.core.windows.net/data-departments/\").orderBy(col(\"dep_id\")).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e2bc60-5b09-4479-954b-b1367681bbb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Definimos las credenciales de acceso a la Base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afb5850-9396-4363-983d-dcc3cf05c547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user = 'admin'\n",
    "password = 'd/¿12la*Qc1tr__.K'\n",
    "name_bd = 'dwh_tp'\n",
    "name_tableEmp = 'employees'\n",
    "name_tableJob = 'jobs'\n",
    "name_tableDep = 'departments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318bdade-2671-4b18-aa6b-9b912125e8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Generamos una url de conexion hacia la base de datos de MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c495c7a-b2e7-43eb-93f2-331b51ccd690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = f\"mysql+mysqlconnector://{user}:{password}@test-facapi-da12sol. 2A122*w3e__wbr2*y.mx-eastus-1.mysql.amazonaws.com:3306/{name_bd}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf9cba5a-d0b4-4d1c-84bc-ff0a4aa3fabf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Escribimos los dataframes de pandas en la base de datos de MySQL utilizando el metodo to_sql(), donde definimos la cadena de conexion, el nombre de la tabla que va a recibir la informacion, el parametro if_exists='replace' e index=False los cuales indican que si las tablas ya existen estas  deben ser reemplazadas, y que no se debe agregar el índice de fila como una columna en la base de datos, este proceso lo realizamos para los 3 dataframes que fueron leidos desde el azure blob storage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943f1cc5-dfb0-4385-b8db-7b8956e07fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 100"
     ]
    }
   ],
   "source": [
    "dfemployees.to_sql(name_tableEmp, url, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5cf5b2-34ec-4c84-b755-0c1d24a79702",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 5"
     ]
    }
   ],
   "source": [
    "dfjobs.to_sql(name_tableJob, url, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefde587-8d3c-4df2-a1f5-6bf5a353529c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 5"
     ]
    }
   ],
   "source": [
    "dfdepartments.to_sql(name_tableDep, url, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef2f536-7709-450d-8476-37c3c9d00cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TestMVM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
